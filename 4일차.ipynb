{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 역전파의 구현과 확률적 경사하강법 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "저번 시간에 구현했던 MNIST 데이터셋은 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, cv2\n",
    "class mnist_dataset():\n",
    "    def __init__(self, path):\n",
    "        data_list = []\n",
    "        for label in range(10):\n",
    "            label_path = os.path.join(path, str(label))\n",
    "            file_list = os.listdir(label_path)\n",
    "            data_list += [[os.path.join(label_path, file), label] for file in file_list]\n",
    "        self.data_list = data_list\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "    def __getitem__(self, i):\n",
    "        png_path, label = self.data_list[i]\n",
    "        img_loaded = cv2.imread(png_path, cv2.IMREAD_GRAYSCALE)\n",
    "        return img_loaded.reshape(-1)/255, label\n",
    "training_dataset = mnist_dataset('../data/mnist_png/training/')\n",
    "test_dataset = mnist_dataset('../data/mnist_png/testing/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이를 이용해 배치를 생성하는 클래스 dataloader를 다음과 구현합니다.     \n",
    "dataloader는 임의의 dataset을 입력으로 받아 배치들을 생성해 주는 함수입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 5 4 8 7 9 0 6 1]\n"
     ]
    }
   ],
   "source": [
    "x = np.arange(10)\n",
    "np.random.shuffle(x)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataloader():\n",
    "    def __init__(self, dataset, batch_size):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.length = len(dataset)\n",
    "        self.index = np.arange(self.length)\n",
    "        np.random.shuffle(self.index)\n",
    "    def __getitem__(self, i):\n",
    "        if i == len(self) - 1:\n",
    "            self.shuffle()\n",
    "        return [self.dataset[self.index[j]] for j in range(i * self.batch_size, (i+1) * self.batch_size)]\n",
    "    def __len__(self):\n",
    "        return self.length//self.batch_size\n",
    "    def shuffle(self):\n",
    "        np.random.shuffle(self.index)\n",
    "        \n",
    "training_dataloader = dataloader(training_dataset, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 3750/3750 [00:08<00:00, 448.72it/s]\n"
     ]
    }
   ],
   "source": [
    "for batch in tqdm(training_dataloader):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "저번 시간에 구현했던 MLP 네트워크와 각종 함수들은 다음과 같습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.48026437  4.16774204 -7.50866318  5.09699204 -3.80197689 -1.49635146\n",
      "  2.08351428  0.48658552 -3.3346366  -2.26909525]\n",
      "-4.095625143264587\n",
      "[1.95928009e-07 4.80767971e-02 1.49119696e-06 2.88211066e-06\n",
      " 8.77502455e-01 7.95170055e-09 3.32523136e-05 1.43003059e-07\n",
      " 4.43998822e-03 6.99427870e-02]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "def ReLU(x):\n",
    "    return x * (x > 0)\n",
    "\n",
    "def no_activation(x):\n",
    "    return x\n",
    "\n",
    "def softmax_temp(z): \n",
    "    return np.exp(z) / np.sum(np.exp(z))\n",
    "\n",
    "def softmax(z):\n",
    "    y = z- np.max(z)\n",
    "    return np.exp(y) / np.sum(np.exp(y))\n",
    "\n",
    "def cross_entropy_loss(y, y_hat):\n",
    "    return np.sum( -y * np.log(y_hat))\n",
    "\n",
    "class MLP_Network:\n",
    "    def __init__(self, sizes, activation_function = ReLU, last_activation = no_activation):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y) for y in sizes[1:]]\n",
    "        self.weights = []\n",
    "        for i in range(self.num_layers-1):\n",
    "            self.weights.append(np.random.randn(self.sizes[i+1], self.sizes[i]))\n",
    "        self.activation_function = ReLU\n",
    "        self.last_activation = last_activation\n",
    "    def forward(self, a):\n",
    "        for i, [bias, weight] in enumerate(zip(self.biases, self.weights)):\n",
    "            if i == self.num_layers - 2:\n",
    "                activation = self.last_activation\n",
    "            else:\n",
    "                activation = self.activation_function\n",
    "            a = activation(np.dot(weight, a)+bias)\n",
    "            \n",
    "        return a\n",
    "    \n",
    "    def accuracy(self,dataset):\n",
    "        num_correct = 0\n",
    "        for i, [img, label] in enumerate(dataset):\n",
    "            y = eye[label]\n",
    "            y_hat = self.forward(img.reshape(-1))\n",
    "            if np.argmax(y_hat) == label:\n",
    "                num_correct += 1\n",
    "        print(f'Accuracy is {num_correct/len(dataset)}' )\n",
    "        \n",
    "\n",
    "    \n",
    "net = MLP_Network([2, 3, 6, 4, 10, 10], activation_function = sigmoid)\n",
    "output = net.forward(np.random.randn((2)))\n",
    "print(output)\n",
    "print(np.sum(output))\n",
    "\n",
    "net = MLP_Network([2, 3, 6, 4, 10, 10], activation_function = sigmoid, last_activation = softmax)\n",
    "output = net.forward(np.random.randn((2)))\n",
    "print(output)\n",
    "print(np.sum(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "역전파의 구현의 편의를 위해 활성화 함수는 ReLU로 마지막 활성화 함수는 소프트맥스로 고정한 후 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_Network:\n",
    "    def __init__(self, sizes):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y) for y in sizes[1:]]\n",
    "        self.weights = []\n",
    "        for i in range(self.num_layers-1):\n",
    "            self.weights.append(np.random.randn(self.sizes[i+1], self.sizes[i]))\n",
    "        self.activation_function = ReLU\n",
    "        self.last_activation = softmax\n",
    "    def forward(self, a):\n",
    "        for i, [bias, weight] in enumerate(zip(self.biases, self.weights)):\n",
    "            if i == self.num_layers - 2:\n",
    "                activation = self.last_activation\n",
    "            else:\n",
    "                activation = self.activation_function\n",
    "            a = activation(np.dot(weight, a)+bias)\n",
    "            \n",
    "        return a\n",
    "    \n",
    "    def accuracy(self, dataset):\n",
    "        num_correct = 0\n",
    "        for i, [img, label] in enumerate(dataset):\n",
    "            y = eye[label]\n",
    "            y_hat = self.forward(img.reshape(-1))\n",
    "            if np.argmax(y_hat) == label:\n",
    "                num_correct += 1\n",
    "        print(f'Accuracy is {num_correct/len(dataset)}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 역전파를 위한 함수를 구현합니다.    \n",
    "수업시간에 다뤘던 대로 $\\frac{\\partial C}{\\partial a^{L}}$를 계산해야 합니다.    \n",
    "수업시간에는 L2 손실함수를 사용하였기 때문에 $\\frac{\\partial C}{\\partial a^{L}} = -(y-x)$였습니다.    \n",
    "하지만 교차 엔트로피 함수를 손실함수로 사용하면 이 미분값도 다시 계산해야 합니다.\n",
    "\n",
    "마지막 $L$층에서의 출력값은 $a^L = softmax(z^L)$가 되고 이를 교차 엔트로피 손실함수에 입력하면 $C = \\sum ^n _{i=1} -y_{i}\\textrm{log}(a^L_i)$가 됩니다.     \n",
    " $\\frac{\\partial C}{\\partial z^{L}}$을 계산하면 $y_i=1$인 $i$에 대해 $\\frac{\\partial C}{\\partial z^{L}} = a^L_i - y_i$를 얻을 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 역전파 진행하는데 앞서 순전파를 진행하여 필요한 $z^l$과 $a^l$를 계산합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(self, x, y):\n",
    "    y = self.eye[y]\n",
    "    grad_bs = [np.zeros(b.shape) for b in self.biases]\n",
    "    grad_ws = [np.zeros(w.shape) for w in self.weights]\n",
    "    z_s = []\n",
    "    a_s = [x]\n",
    "    a = x\n",
    "    for i, [bias, weight] in enumerate(zip(self.biases, self.weights)):\n",
    "        if i == self.num_layers - 2:\n",
    "            activation = self.last_activation\n",
    "        else:\n",
    "            activation = self.activation_function\n",
    "        z = np.matmul(weight, a)+bias\n",
    "        a = activation(z)\n",
    "        z_s.append(z)\n",
    "        a_s.append(a)\n",
    "    dz = a_s[-1] - y \n",
    "    grad_bs[-1] = dz\n",
    "    grad_ws[-1] = np.matmul(dz.reshape(-1, 1), a_s[-2].reshape(1, -1))\n",
    "    for l in range(2, self.num_layers):\n",
    "        z = z_s[-l]\n",
    "        dz = np.matmul(self.weights[-l+1].transpose(), dz) * self.ReLU_prime(z)\n",
    "        grad_bs[-l] = dz\n",
    "        grad_ws[-l] = np.matmul(dz.reshape(-1,1), a_s[-l-1].reshape(1,-1))\n",
    "    return [grad_bs, grad_ws]\n",
    "\n",
    "    \n",
    "def ReLU_prime(self, z):\n",
    "    return (z>0).astype(np.int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각각의 $(x_i, y_i)$ 대해 경사를 계산 후 이를 배치 전체의 데이터에 대해 더해 전체 경사를 계산합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_mini_batch(self, batch):\n",
    "    sum_grad_b = [np.zeros(b.shape) for b in self.biases]\n",
    "    sum_grad_w = [np.zeros(w.shape) for w in self.weights]\n",
    "    for x, y in mini_batch:\n",
    "        grad_b, grad_w = self.backprop(x, y)\n",
    "        sum_grad_b = [sgb + gb for sgb, gb in zip(sum_grad_b, grad_b)]\n",
    "        sum_grad_w = [sgw + gw for sgw, gw in zip(sum_grad_w, grad_w)]\n",
    "    return sum_grad_b, sum_grad_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이를 이용해 배치와 학습률 $\\lambda$를 입력으로 받아 파라미터를 업데이트하는 함수를 다음과 같이 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_mini_batch(self, batch, lambda_):\n",
    "    n = len(mini_batch)\n",
    "    sum_grad_b, sum_grad_w = self.grad_mini_batch(batch)\n",
    "    self.weigths = [w- (lambda_/n) * sgb  for w, sgb in zip(self.weights, sum_grad_b)]\n",
    "    self.biases = [b- (lambda_/n) * sgw  for b, sgw in zip(self.biases, sum_grad_w)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "배치에 대해 계산한 경사를 이용해 확률적 경사 하강법을 다음과 같이 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(self, dataset, epochs, batch_size, lambda_):\n",
    "    training_dataloader = dataloader(dataset, batch_size)\n",
    "    n = len(training_data)\n",
    "    for j in range(epochs):\n",
    "        for mini_batch in training_dataloader:\n",
    "            self.update_mini_batch(mini_batch, lambda_)\n",
    "        print(f'Epoch {j} complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금까지 구현한 결과를 통합하면 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_Network:\n",
    "    def __init__(self, sizes):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y) for y in sizes[1:]]\n",
    "        self.weights = []\n",
    "        for i in range(self.num_layers-1):\n",
    "            self.weights.append(np.random.randn(self.sizes[i+1], self.sizes[i]))\n",
    "        self.activation_function = ReLU\n",
    "        self.last_activation = softmax\n",
    "        self.eye = np.eye(sizes[-1])\n",
    "    def forward(self, a):\n",
    "        for i, [bias, weight] in enumerate(zip(self.biases, self.weights)):\n",
    "            if i == self.num_layers - 2:\n",
    "                activation = self.last_activation\n",
    "            else:\n",
    "                activation = self.activation_function\n",
    "            a = activation(np.matmul(weight, a)+bias)\n",
    "        return a\n",
    "    \n",
    "    def accuracy(self,dataset):\n",
    "        num_correct = 0\n",
    "        for i, [img, label] in enumerate(dataset):\n",
    "            y = self.eye[label]\n",
    "            y_hat = self.forward(img)\n",
    "            if np.argmax(y_hat) == label:\n",
    "                num_correct += 1\n",
    "        print(f'Accuracy is {num_correct/len(dataset)}' )\n",
    "    def backward(self, x, y):\n",
    "        y = self.eye[y]\n",
    "        grad_bs = [np.zeros(b.shape) for b in self.biases]\n",
    "        grad_ws = [np.zeros(w.shape) for w in self.weights]\n",
    "        z_s = []\n",
    "        a_s = [x]\n",
    "        a = x\n",
    "        for i, [bias, weight] in enumerate(zip(self.biases, self.weights)):\n",
    "            if i == self.num_layers - 2:\n",
    "                activation = self.last_activation\n",
    "            else:\n",
    "                activation = self.activation_function\n",
    "            z = np.matmul(weight, a)+bias\n",
    "            a = activation(z)\n",
    "            z_s.append(z)\n",
    "            a_s.append(a)\n",
    "        dz = a_s[-1] - y \n",
    "        grad_bs[-1] = dz\n",
    "        grad_ws[-1] = np.matmul(dz.reshape(-1, 1), a_s[-2].reshape(1, -1))\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = z_s[-l]\n",
    "            dz = np.matmul(self.weights[-l+1].transpose(), dz) * self.ReLU_prime(z)\n",
    "            grad_bs[-l] = dz\n",
    "            grad_ws[-l] = np.matmul(dz.reshape(-1,1), a_s[-l-1].reshape(1,-1))\n",
    "        return [grad_bs, grad_ws]\n",
    "\n",
    "    def ReLU_prime(self, z):\n",
    "        return (z>0).astype(np.int)\n",
    "    \n",
    "    def grad_batch(self, batch):\n",
    "        sum_grad_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        sum_grad_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in batch:\n",
    "            grad_b, grad_w = self.backward(x, y)\n",
    "            sum_grad_b = [sgb + gb for sgb, gb in zip(sum_grad_b, grad_b)]\n",
    "            sum_grad_w = [sgw + gw for sgw, gw in zip(sum_grad_w, grad_w)]\n",
    "        return sum_grad_b, sum_grad_w\n",
    "    \n",
    "    def update_batch(self, batch, lambda_):\n",
    "        n = len(batch)\n",
    "        sum_grad_b, sum_grad_w = self.grad_batch(batch)\n",
    "        self.weights = [w- (lambda_/n) * sgb  for w, sgb in zip(self.weights, sum_grad_w)]\n",
    "        self.biases = [b- (lambda_/n) * sgw  for b, sgw in zip(self.biases, sum_grad_b)]\n",
    "        \n",
    "    def SGD(self, dataset, epochs, batch_size, lambda_):\n",
    "        training_dataloader = dataloader(dataset, batch_size)\n",
    "        for j in range(epochs):\n",
    "            for mini_batch in training_dataloader:\n",
    "                self.update_batch(mini_batch, lambda_)\n",
    "                \n",
    "            print(f'Epoch {j} complete')\n",
    "            self.accuracy(dataset)\n",
    "            self.loss_dataset(dataset)\n",
    "            \n",
    "    def loss(self, img, label):\n",
    "        y = self.eye[label]\n",
    "        y_hat = self.forward(img)\n",
    "        loss = cross_entropy_loss(y, y_hat)\n",
    "        return loss\n",
    "    \n",
    "    def loss_dataset(self, dataset):\n",
    "        loss_sum = 0\n",
    "        for i, [img, label] in enumerate(dataset):\n",
    "            loss = self.loss(img, label)\n",
    "            loss_sum += loss\n",
    "        print(f'Loss is {loss_sum/len(dataset)}')\n",
    "        return loss_sum/len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mlp_network = MLP_Network([784, 512, 10])\n",
    "mlp_network.SGD(training_dataset, 3, 10, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
