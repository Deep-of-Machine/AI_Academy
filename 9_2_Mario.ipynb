{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "9-2_Mario.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Deep-of-Machine/AI_Academy/blob/main/9_2_Mario.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfOe7eDJYsCF"
      },
      "source": [
        "# 슈퍼 마리오 브라더스 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHtulhifTR2u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6837634-4273-469d-c21c-261388c9db74"
      },
      "source": [
        "!pip install gym-super-mario-bros==7.3.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gym-super-mario-bros==7.3.0\n",
            "  Downloading gym_super_mario_bros-7.3.0-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 5.0 MB/s \n",
            "\u001b[?25hCollecting nes-py>=8.0.0\n",
            "  Downloading nes_py-8.1.8.tar.gz (76 kB)\n",
            "\u001b[K     |████████████████████████████████| 76 kB 2.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gym>=0.17.2 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (1.19.5)\n",
            "Requirement already satisfied: pyglet<=1.5.11,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (1.5.0)\n",
            "Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (4.62.3)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.11,>=1.4.0->nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (0.16.0)\n",
            "Building wheels for collected packages: nes-py\n",
            "  Building wheel for nes-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nes-py: filename=nes_py-8.1.8-cp37-cp37m-linux_x86_64.whl size=438939 sha256=76501ee2a9f12ed818c889341697d07a7092242cfa780774e1007e61da1e9caa\n",
            "  Stored in directory: /root/.cache/pip/wheels/f2/05/1f/608f15ab43187096eb5f3087506419c2d9772e97000f3ba025\n",
            "Successfully built nes-py\n",
            "Installing collected packages: nes-py, gym-super-mario-bros\n",
            "Successfully installed gym-super-mario-bros-7.3.0 nes-py-8.1.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyEtXrv7S5Gj"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import transforms as T\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from collections import deque\n",
        "import random, datetime, os, copy, time\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Gym is an OpenAI toolkit for RL\n",
        "import gym\n",
        "from gym.spaces import Box\n",
        "from gym.wrappers import FrameStack\n",
        "\n",
        "# NES Emulator for OpenAI Gym\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "\n",
        "# Super Mario environment for OpenAI Gym\n",
        "import gym_super_mario_bros"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdHUwaFOS5Gl"
      },
      "source": [
        "## 환경 불러오기\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFdPef2vqs94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f7e2801-0447-485e-aa96-cee37f2fb20b"
      },
      "source": [
        "env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\")\n",
        "print(env.observation_space)\n",
        "print(env.action_space)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Box(0, 255, (240, 256, 3), uint8)\n",
            "Discrete(256)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_icXJXgqwc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e64e4483-9964-41c3-8f08-3b21d8cce31b"
      },
      "source": [
        "state = env.reset()\n",
        "print(type(state))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4UhwfwZdGNR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e35fe1f-0348-4b79-dbbe-986e8b24bbc3"
      },
      "source": [
        "print(state.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(240, 256, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0APxQSzMq9JN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "5ae90ec7-7024-497b-9351-8c2e2d201951"
      },
      "source": [
        "plt.imshow(state)\n",
        "plt.show()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JL6P0DvdbGtC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac383787-e95f-40af-dc3a-2f77c3604c70"
      },
      "source": [
        "one_step = env.step(10)\n",
        "print(type(one_step))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'tuple'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eZThVcpda9w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cf75538-fa52-4d88-80d3-5062c3a1d1b1"
      },
      "source": [
        "len(one_step)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbry9atZdlcF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f065194-ee89-4a0f-fe3d-a25e2106c278"
      },
      "source": [
        "print(one_step[0].shape)\n",
        "print(one_step[1])\n",
        "print(one_step[2])\n",
        "print(one_step[3])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(240, 256, 3)\n",
            "0\n",
            "False\n",
            "{'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'x_pos_screen': 40, 'y_pos': 79}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Y3qEpL6rB3j"
      },
      "source": [
        "## 액션 간소화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loBp18As1ey-"
      },
      "source": [
        "env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7uqAC8Um_Yv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5efbf2f-6e63-496b-ba4e-38be2df6fdcd"
      },
      "source": [
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
        "SIMPLE_MOVEMENT"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['NOOP'],\n",
              " ['right'],\n",
              " ['right', 'A'],\n",
              " ['right', 'B'],\n",
              " ['right', 'A', 'B'],\n",
              " ['A'],\n",
              " ['left']]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHn7oW9JS5Gm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f91866e-224f-43f5-bf4d-7d409c4b6b4f"
      },
      "source": [
        "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
        "env.reset()\n",
        "next_state, reward, done, info = env.step(action=0)\n",
        "print(env.action_space)\n",
        "print(next_state.shape)\n",
        "print(reward)\n",
        "print(done)\n",
        "print(info)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Discrete(7)\n",
            "(240, 256, 3)\n",
            "0\n",
            "False\n",
            "{'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'x_pos_screen': 40, 'y_pos': 79}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvhX5eU-ZCmT"
      },
      "source": [
        "7개의 액션을 무작위로 선택한다면?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cV41AJ-iiuN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9be11f28-592c-4c4e-8b7e-a40e72eb0d95"
      },
      "source": [
        "0.1*0.95**200"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.5052666248828703e-06"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xi_k_JoBZLuZ"
      },
      "source": [
        "더 간단하게 2개의 액션을 사용한다면?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "do8kpz5Y1rhS"
      },
      "source": [
        "env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\")\n",
        "SIMPLE_MOVEMENT = [['right'], ['right', 'A']]\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SplkkCkxS5Gm"
      },
      "source": [
        "## 환경 전처리    \n",
        "SkipFrame : 중간의 프레임 스킵    \n",
        "GrayScaleObservation : 컬러 이미지를 흑백으로 변환    \n",
        "ResizeObservation : 240 x 256 크기의 이미지를 84 x 84 크기로 변환    \n",
        "FrameStack : 이전 프레임까지 포함해서 총 4장의 이미지를 반환"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2w9_WsbtS5Go"
      },
      "source": [
        "class SkipFrame(gym.Wrapper):\n",
        "    def __init__(self, env, skip):\n",
        "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
        "        super().__init__(env)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Repeat action, and sum reward\"\"\"\n",
        "        total_reward = 0.0\n",
        "        done = False\n",
        "        for i in range(self._skip):\n",
        "            # Accumulate reward and repeat the same action\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        return obs, total_reward, done, info\n",
        "\n",
        "\n",
        "class GrayScaleObservation(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        obs_shape = self.observation_space.shape[:2]\n",
        "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
        "\n",
        "    def permute_orientation(self, observation):\n",
        "        # permute [H, W, C] array to [C, H, W] tensor\n",
        "        observation = np.transpose(observation, (2, 0, 1))\n",
        "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
        "        return observation\n",
        "\n",
        "    def observation(self, observation):\n",
        "        observation = self.permute_orientation(observation)\n",
        "        transform = T.Grayscale()\n",
        "        observation = transform(observation)\n",
        "        return observation\n",
        "\n",
        "\n",
        "class ResizeObservation(gym.ObservationWrapper):\n",
        "    def __init__(self, env, shape):\n",
        "        super().__init__(env)\n",
        "        if isinstance(shape, int):\n",
        "            self.shape = (shape, shape)\n",
        "        else:\n",
        "            self.shape = tuple(shape)\n",
        "\n",
        "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
        "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        transforms = T.Compose(\n",
        "            [T.Resize(self.shape), T.Normalize(0, 255)]\n",
        "        )\n",
        "        observation = transforms(observation).squeeze(0)\n",
        "        return observation\n",
        "\n",
        "\n",
        "# Apply Wrappers to environment\n",
        "env = SkipFrame(env, skip=4)\n",
        "env = GrayScaleObservation(env)\n",
        "env = ResizeObservation(env, shape=84)\n",
        "env = FrameStack(env, num_stack=4)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiqjqxjgs9s1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c06dd32-f439-424c-e162-826c148f5ce2"
      },
      "source": [
        "state = env.reset()\n",
        "print(state.shape)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4, 84, 84)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRVPTIersWKo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dk56ydMS5Gq"
      },
      "source": [
        "## 학습할 에이전트 클래스\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmaYpbDGS5Gq"
      },
      "source": [
        "class Mario:\n",
        "    def __init__():\n",
        "        pass\n",
        "\n",
        "    def act(self, state):\n",
        "        \"\"\"Given a state, choose an epsilon-greedy action\"\"\"\n",
        "        pass\n",
        "\n",
        "    def cache(self, experience):\n",
        "        \"\"\"Add the experience to memory\"\"\"\n",
        "        pass\n",
        "\n",
        "    def recall(self):\n",
        "        \"\"\"Sample experiences from memory\"\"\"\n",
        "        pass\n",
        "\n",
        "    def learn(self):\n",
        "        \"\"\"Update online action value (Q) function with a batch of experiences\"\"\"\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTiRCWyWS5Gr"
      },
      "source": [
        "## Act\n",
        "상태가 주어졌을 때, 입실론 그리디 방법으로 취할 액션을 선택    \n",
        "입실론의 값을 조금씩 바꿔서 점점 이용을 증가\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeaG5SnwS5Gr"
      },
      "source": [
        "class Mario:\n",
        "    def __init__(self, state_dim, action_dim, save_dir):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.save_dir = save_dir\n",
        "\n",
        "        self.use_cuda = torch.cuda.is_available()\n",
        "\n",
        "        # Mario's DNN to predict the most optimal action - we implement this in the Learn section\n",
        "        self.net = MarioNet(self.state_dim, self.action_dim).float()\n",
        "        if self.use_cuda:\n",
        "            self.net = self.net.to(device=\"cuda\")\n",
        "\n",
        "        self.exploration_rate = 1\n",
        "        self.exploration_rate_decay = 0.99999975\n",
        "        self.exploration_rate_min = 0.1\n",
        "        self.curr_step = 0\n",
        "\n",
        "        self.save_every = 5e5  # no. of experiences between saving Mario Net\n",
        "\n",
        "    def act(self, state):\n",
        "        # EXPLORE\n",
        "        if np.random.rand() < self.exploration_rate:\n",
        "            action_idx = np.random.randint(self.action_dim)\n",
        "\n",
        "        # EXPLOIT\n",
        "        else:\n",
        "            state = state.__array__()\n",
        "            if self.use_cuda:\n",
        "                state = torch.tensor(state).cuda()\n",
        "            else:\n",
        "                state = torch.tensor(state)\n",
        "            state = state.unsqueeze(0)\n",
        "            action_values = self.net(state, model=\"online\")\n",
        "            action_idx = torch.argmax(action_values, axis = 1).item()\n",
        "        # decrease exploration_rate\n",
        "        self.exploration_rate *= self.exploration_rate_decay\n",
        "        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
        "        \n",
        "        # increment step\n",
        "        self.curr_step += 1\n",
        "        return action_idx"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPFgf36wS5Gr"
      },
      "source": [
        "## Cache and Recall\n",
        "액션을 취해서 상태가 변하고 보상을 받으면, 그 결과를 저장    \n",
        "저장된 경험에서 한 번의 학습에 사용 할 배치를 가져오기\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNDAq2KmS5Gs"
      },
      "source": [
        "class Mario(Mario):  # subclassing for continuity\n",
        "    def __init__(self, state_dim, action_dim, save_dir):\n",
        "        super().__init__(state_dim, action_dim, save_dir)\n",
        "        self.memory = deque(maxlen=30000)\n",
        "        self.batch_size = 32\n",
        "\n",
        "    def cache(self, state, next_state, action, reward, done):\n",
        "        \n",
        "        state = state.__array__()\n",
        "        next_state = next_state.__array__()\n",
        "\n",
        "        if self.use_cuda:\n",
        "            state = torch.tensor(state).cuda()\n",
        "            next_state = torch.tensor(next_state).cuda()\n",
        "            action = torch.tensor([action]).cuda()\n",
        "            reward = torch.tensor([reward]).cuda()\n",
        "            done = torch.tensor([done]).cuda()\n",
        "        else:\n",
        "            state = torch.tensor(state)\n",
        "            next_state = torch.tensor(next_state)\n",
        "            action = torch.tensor([action])\n",
        "            reward = torch.tensor([reward])\n",
        "            done = torch.tensor([done])\n",
        "\n",
        "        self.memory.append((state, next_state, action, reward, done,))\n",
        "\n",
        "    def recall(self):\n",
        "\n",
        "        batch = random.sample(self.memory, self.batch_size)\n",
        "        state, next_state, action, reward, done = map(torch.stack, zip(*batch))\n",
        "        return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpcdRnz-S5Gs"
      },
      "source": [
        "## 학습에 사용할 모델(DDQN) 정의\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v65K95ZhS5Gs"
      },
      "source": [
        "class MarioNet(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        c, h, w = input_dim\n",
        "\n",
        "        if h != 84:\n",
        "            raise ValueError(f\"Expecting input height: 84, got: {h}\")\n",
        "        if w != 84:\n",
        "            raise ValueError(f\"Expecting input width: 84, got: {w}\")\n",
        "\n",
        "        self.online = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(3136, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, output_dim),\n",
        "        )\n",
        "\n",
        "        self.target = copy.deepcopy(self.online)\n",
        "\n",
        "        # Q_target parameters are frozen.\n",
        "        for p in self.target.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "    def forward(self, input, model):\n",
        "        if model == \"online\":\n",
        "            return self.online(input)\n",
        "        elif model == \"target\":\n",
        "            return self.target(input)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iq2aGHxzS5Gt"
      },
      "source": [
        "class Mario(Mario):\n",
        "    def __init__(self, state_dim, action_dim, save_dir):\n",
        "        super().__init__(state_dim, action_dim, save_dir)\n",
        "        self.gamma = 0.9\n",
        "\n",
        "    def td_estimate(self, state, action):\n",
        "        current_Q = self.net(state, model=\"online\")[\n",
        "            np.arange(0, self.batch_size), action\n",
        "        ]  # Q_online(s,a)\n",
        "        return current_Q\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def td_target(self, reward, next_state, done):\n",
        "        next_state_Q = self.net(next_state, model=\"online\")\n",
        "        best_action = torch.argmax(next_state_Q, axis=1)\n",
        "        next_Q = self.net(next_state, model=\"target\")[\n",
        "            np.arange(0, self.batch_size), best_action\n",
        "        ]\n",
        "        return (reward + (1 - done.float()) * self.gamma * next_Q).float()"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtdmQ_PBS5Gt"
      },
      "source": [
        "## 위에서 정의한 모델 학습\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnwpnAKcS5Gu"
      },
      "source": [
        "class Mario(Mario):\n",
        "    def __init__(self, state_dim, action_dim, save_dir):\n",
        "        super().__init__(state_dim, action_dim, save_dir)\n",
        "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025)\n",
        "        self.loss_fn = torch.nn.SmoothL1Loss()\n",
        "\n",
        "    def update_Q_online(self, td_estimate, td_target):\n",
        "        loss = self.loss_fn(td_estimate, td_target)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        return loss.item()\n",
        "\n",
        "\n",
        "    def sync_Q_target(self):\n",
        "        self.net.target.load_state_dict(self.net.online.state_dict())"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b2SE7wbdGoI"
      },
      "source": [
        "## 학습한 모델 저장"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dwGjqCcS5Gu"
      },
      "source": [
        "class Mario(Mario):\n",
        "    def save(self):\n",
        "        save_path = (self.save_dir / f\"mario_net_{int(self.curr_step // self.save_every)}.pth\")\n",
        "        torch.save(self.net.state_dict(), save_path)\n",
        "        print(f\"MarioNet saved to {save_path} at step {self.curr_step}\")"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVhA85EXS5Gu"
      },
      "source": [
        "## Learn\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6gB34_iS5Gu"
      },
      "source": [
        "class Mario(Mario):\n",
        "    def __init__(self, state_dim, action_dim, save_dir):\n",
        "        super().__init__(state_dim, action_dim, save_dir)\n",
        "        self.burnin = 1e4  # min. experiences before training\n",
        "        self.learn_every = 3  # no. of experiences between updates to Q_online\n",
        "        self.sync_every = 1e4  # no. of experiences between Q_target & Q_online sync\n",
        "\n",
        "    def learn(self):\n",
        "        if self.curr_step % self.sync_every == 0:\n",
        "            self.sync_Q_target()\n",
        "\n",
        "        if self.curr_step % self.save_every == 0:\n",
        "            self.save()\n",
        "\n",
        "        if self.curr_step < self.burnin:\n",
        "            return None, None\n",
        "\n",
        "        if self.curr_step % self.learn_every != 0:\n",
        "            return None, None\n",
        "\n",
        "        # Sample from memory\n",
        "        state, next_state, action, reward, done = self.recall()\n",
        "\n",
        "        # Get TD Estimate\n",
        "        td_est = self.td_estimate(state, action)\n",
        "        # Get TD Target\n",
        "        td_tgt = self.td_target(reward, next_state, done)\n",
        "\n",
        "        # Backpropagate loss through Q_online\n",
        "        loss = self.update_Q_online(td_est, td_tgt)\n",
        "\n",
        "        return (td_est.mean().item(), loss)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qo6Wc0XsS5Gu"
      },
      "source": [
        "## Logging\n",
        "학습 결과를 시각화하고 저장"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZkdT90yS5Gv"
      },
      "source": [
        "class MetricLogger:\n",
        "    def __init__(self, save_dir):\n",
        "        self.save_log = save_dir / \"log\"\n",
        "        with open(self.save_log, \"w\") as f:\n",
        "            f.write(\n",
        "                f\"{'Episode':>8}{'Step':>8}{'Epsilon':>10}{'MeanReward':>15}\"\n",
        "                f\"{'MeanLength':>15}{'MeanLoss':>15}{'MeanQValue':>15}\"\n",
        "                f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n",
        "            )\n",
        "        self.ep_rewards_plot = save_dir / \"reward_plot.jpg\"\n",
        "        self.ep_lengths_plot = save_dir / \"length_plot.jpg\"\n",
        "        self.ep_avg_losses_plot = save_dir / \"loss_plot.jpg\"\n",
        "        self.ep_avg_qs_plot = save_dir / \"q_plot.jpg\"\n",
        "\n",
        "        # History metrics\n",
        "        self.ep_rewards = []\n",
        "        self.ep_lengths = []\n",
        "        self.ep_avg_losses = []\n",
        "        self.ep_avg_qs = []\n",
        "\n",
        "        # Moving averages, added for every call to record()\n",
        "        self.moving_avg_ep_rewards = []\n",
        "        self.moving_avg_ep_lengths = []\n",
        "        self.moving_avg_ep_avg_losses = []\n",
        "        self.moving_avg_ep_avg_qs = []\n",
        "\n",
        "        # Current episode metric\n",
        "        self.init_episode()\n",
        "\n",
        "        # Timing\n",
        "        self.record_time = time.time()\n",
        "\n",
        "    def log_step(self, reward, loss, q):\n",
        "        self.curr_ep_reward += reward\n",
        "        self.curr_ep_length += 1\n",
        "        if loss:\n",
        "            self.curr_ep_loss += loss\n",
        "            self.curr_ep_q += q\n",
        "            self.curr_ep_loss_length += 1\n",
        "\n",
        "    def log_episode(self):\n",
        "        \"Mark end of episode\"\n",
        "        self.ep_rewards.append(self.curr_ep_reward)\n",
        "        self.ep_lengths.append(self.curr_ep_length)\n",
        "        if self.curr_ep_loss_length == 0:\n",
        "            ep_avg_loss = 0\n",
        "            ep_avg_q = 0\n",
        "        else:\n",
        "            ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n",
        "            ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n",
        "        self.ep_avg_losses.append(ep_avg_loss)\n",
        "        self.ep_avg_qs.append(ep_avg_q)\n",
        "\n",
        "        self.init_episode()\n",
        "\n",
        "    def init_episode(self):\n",
        "        self.curr_ep_reward = 0.0\n",
        "        self.curr_ep_length = 0\n",
        "        self.curr_ep_loss = 0.0\n",
        "        self.curr_ep_q = 0.0\n",
        "        self.curr_ep_loss_length = 0\n",
        "\n",
        "    def record(self, episode, epsilon, step):\n",
        "        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n",
        "        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n",
        "        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n",
        "        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n",
        "        self.moving_avg_ep_rewards.append(mean_ep_reward)\n",
        "        self.moving_avg_ep_lengths.append(mean_ep_length)\n",
        "        self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n",
        "        self.moving_avg_ep_avg_qs.append(mean_ep_q)\n",
        "\n",
        "        last_record_time = self.record_time\n",
        "        self.record_time = time.time()\n",
        "        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n",
        "\n",
        "        print(\n",
        "            f\"Episode {episode} - \"\n",
        "            f\"Step {step} - \"\n",
        "            f\"Epsilon {epsilon} - \"\n",
        "            f\"Mean Reward {mean_ep_reward} - \"\n",
        "            f\"Mean Length {mean_ep_length} - \"\n",
        "            f\"Mean Loss {mean_ep_loss} - \"\n",
        "            f\"Mean Q Value {mean_ep_q} - \"\n",
        "            f\"Time Delta {time_since_last_record} - \"\n",
        "            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n",
        "        )\n",
        "\n",
        "        with open(self.save_log, \"a\") as f:\n",
        "            f.write(\n",
        "                f\"{episode:8d}{step:8d}{epsilon:10.3f}\"\n",
        "                f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}{mean_ep_loss:15.3f}{mean_ep_q:15.3f}\"\n",
        "                f\"{time_since_last_record:15.3f}\"\n",
        "                f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n",
        "            )\n",
        "\n",
        "        for metric in [\"ep_rewards\", \"ep_lengths\", \"ep_avg_losses\", \"ep_avg_qs\"]:\n",
        "            plt.plot(getattr(self, f\"moving_avg_{metric}\"))\n",
        "            plt.savefig(getattr(self, f\"{metric}_plot\"))\n",
        "            plt.clf()"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMUQsjsdS5Gv"
      },
      "source": [
        "# 학습 시작\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DhILODkCONV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f991c4eb-c8b1-493f-e76f-ab633871c6a3"
      },
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "print(f\"Using CUDA: {use_cuda}\")"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CUDA: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CySBtn1eyYwr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "673b33ab-331f-415f-bdbb-f0c99fb79ac3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "re33izGbS5Gv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "outputId": "17482a01-91d7-44d2-b223-95036e0a7ee4"
      },
      "source": [
        "save_dir = Path('/content/drive/MyDrive/SuperMario_DDQN') / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
        "save_dir.mkdir(parents=True)\n",
        "logger = MetricLogger(save_dir)\n",
        "\n",
        "mario = Mario(state_dim=(4,84,84), action_dim=env.action_space.n, save_dir=save_dir)\n",
        "\n",
        "episodes = 1000000\n",
        "for e in range(1, episodes + 1):\n",
        "\n",
        "    state=env.reset()\n",
        "\n",
        "    while True:\n",
        "        action = mario.act(state)\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        mario.cache(state, next_state, action, reward, done)\n",
        "        q, loss = mario.learn()\n",
        "        logger.log_step(reward, loss, q)\n",
        "        state = next_state\n",
        "        if done or info[\"flag_get\"]:\n",
        "            break\n",
        "\n",
        "    logger.log_episode()\n",
        "\n",
        "    if e % 20 ==0:\n",
        "        logger.record(episode=e, epsilon=mario.exploration_rate, step=mario.curr_step)\n",
        "        mario.save()\n"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym_super_mario_bros/smb_env.py:148: RuntimeWarning: overflow encountered in ubyte_scalars\n",
            "  return (self.ram[0x86] - self.ram[0x071c]) % 256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 20 - Step 39160 - Epsilon 0.9902577648331903 - Mean Reward 515.05 - Mean Length 1958.0 - Mean Loss 0.511 - Mean Q Value 0.258 - Time Delta 511.836 - Time 2021-10-30T07:32:24\n",
            "MarioNet saved to /content/drive/MyDrive/SuperMario_DDQN/2021-10-30T07-23-52/mario_net_0.pth at step 39160\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-a2b8f81ea214>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmario\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mmario\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmario\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/wrappers/frame_stack.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-60eb08335e68>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_skip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;31m# Accumulate reward and repeat the same action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nes_py/nes_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;31m# call the after step callback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_did_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m         \u001b[0;31m# bound the reward in [min, max]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward_range\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym_super_mario_bros/smb_env.py\u001b[0m in \u001b[0;36m_did_step\u001b[0;34m(self, done)\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;31m# skip occupied states like the black screen between lives that shows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0;31m# how many lives the player has left\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_skip_occupied_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym_super_mario_bros/smb_env.py\u001b[0m in \u001b[0;36m_skip_occupied_states\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_busy\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_world_over\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_runout_prelevel_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_frame_advance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_skip_start_screen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nes_py/nes_env.py\u001b[0m in \u001b[0;36m_frame_advance\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrollers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;31m# perform a step on the emulator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_env\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_backup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_-jiH9zAHnM"
      },
      "source": [
        "## 학습된 에이전트 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sU5SFQDmIpqc"
      },
      "source": [
        "luisi = Mario(state_dim=(4,84,84),action_dim=env.action_space.n, save_dir=save_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDZHlx_fI7h7"
      },
      "source": [
        "luisi.net.online"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7r8ZdjxI1mB"
      },
      "source": [
        "luisi.net.load_state_dict(torch.load('드라이브 위치'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Z9aRp9VZzsj"
      },
      "source": [
        "max_step = 5000\n",
        "init_step = 0\n",
        "done = False\n",
        "\n",
        "state = env.reset()\n",
        "while not done:\n",
        "    clear_output(wait=True)\n",
        "    action = luisi.act(state)\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    state =  next_state\n",
        "    plt.imshow(next_state[3], cmap='gray')\n",
        "    plt.show()\n",
        "    init_step +=1\n",
        "    print('\\n {}th action {}, x_pos: {}'.format(init_step, action, info['x_pos']))\n",
        "\n",
        "    if info[\"flag_get\"]:\n",
        "        print(\"Get Flag!\")\n",
        "        break\n",
        "\n",
        "    if done:\n",
        "        print('Fail')\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdtKAt73AKs3"
      },
      "source": [
        "## 저장된 모델 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMxQtm7s5p4D"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QG2xNhgL9Mv1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcAp9klF9fhg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPwl9MMT82hD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJHbNd1e_aO2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vqUP27u9lse"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdymGZOz_UXe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}